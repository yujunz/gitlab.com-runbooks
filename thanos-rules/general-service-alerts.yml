groups:
  - name: slo_alerts.rules
    partial_response_strategy: warn
    rules:
      # ------------------------------------------------------------------------------
      # Latency alerts
      # ------------------------------------------------------------------------------

      # Warn: Latency SLO not being met, default alert_trigger_duration (short, 5m)
      # Note: we ignore `cny` stage for this alert, since if both cny and main stage are firing, we
      # only care about the main stage alert Alternatively, we have the bad canary alerts
      # specifically for canary stage
      # DEPRECATED in favour of multiwindow, multiburn-rate alerts
      - alert: service_apdex_slo_out_of_bounds_lower_5m
        expr: |
          (
            avg(gitlab_service_apdex:ratio{stage!="cny", monitor="global"}) by (environment, tier, type, stage)
            < ignoring(environment, stage) group_left
            avg(slo:min:gitlab_service_apdex:ratio{alert_trigger_duration!="long", monitor="global"}) by (tier, type)
          )
          unless on(tier, type)
          (
            slo:min:events:gitlab_service_apdex:ratio{monitor="global"}
          )
        for: 5m
        labels:
          rules_domain: general
          metric: gitlab_service_apdex:ratio
          severity: s2
          slo_alert: 'yes'
          period: 5m
          bound: lower
          pager: pagerduty
          alert_type: symptom
          deprecated: 'yes'
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is not meeting its latency SLOs

            The service is taking longer to respond to requests than usual. This could be caused by
            user abuse, application changes in upstream services that lead to higher request rates or slower
            requested, or slowdown in downstream services. Check operation rates in upstream and downstream
            services, error rates and check ELK for abuse.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has a apdex score (latency) below SLO"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "7"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-apdex.md"
          promql_template_1: 'gitlab_service_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'

      # Warn: Latency SLO not being met, long alert_trigger_duration (15m)
      # Note: we ignore `cny` stage for this alert, since if both cny and main stage are firing, we
      # only care about the main stage alert Alternatively, we have the bad canary alerts
      # specifically for canary stage
      # DEPRECATED in favour of multiwindow, multiburn-rate alerts
      - alert: service_apdex_slo_out_of_bounds_lower_15m
        expr: |
          (
            avg(gitlab_service_apdex:ratio{stage!="cny", monitor="global"}) by (environment, tier, type, stage)
            < ignoring(environment, stage) group_left
            avg(slo:min:gitlab_service_apdex:ratio{alert_trigger_duration="long", monitor="global"}) by (tier, type)
          )
          unless on(tier, type)
          (
            slo:min:events:gitlab_service_apdex:ratio{monitor="global"}
          )
        for: 15m
        labels:
          rules_domain: general
          metric: gitlab_service_apdex:ratio
          severity: s2
          slo_alert: 'yes'
          period: 15m
          bound: lower
          pager: pagerduty
          alert_type: symptom
          deprecated: 'yes'
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is not meeting its latency SLOs

            The service is taking longer to respond to requests than usual. This could be caused by
            user abuse, application changes in upstream services that lead to higher request rates or slower
            requested, or slowdown in downstream services. Check operation rates in upstream and downstream
            services, error rates and check ELK for abuse.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has a apdex score (latency) below SLO"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "7"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-apdex.md"
          promql_template_1: 'gitlab_service_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'

      # ------------------------------------------------------------------------------
      # Error Rate alerts
      # ------------------------------------------------------------------------------

      # Warn: Error ratio SLO not being met, default alert_trigger_duration (short, 5m)
      # Note: we ignore `cny` stage for this alert, since if both cny and main stage are firing, we
      # only care about the main stage alert Alternatively, we have the bad canary alerts
      # specifically for canary stage
      # DEPRECATED in favour of multiwindow, multiburn-rate alerts
      - alert: service_error_ratio_slo_out_of_bounds_upper_5m
        expr: |
          (
            avg(gitlab_service_errors:ratio{stage!="cny", monitor="global"}) by (environment, tier, type, stage)
            > ignoring(environment, stage) group_left
            avg(slo:max:gitlab_service_errors:ratio{alert_trigger_duration!="long", monitor="global"}) by (tier, type)
          )
          unless on(tier, type)
          (
            slo:max:events:gitlab_service_errors:ratio{monitor="global"}
          )
        for: 5m
        labels:
          rules_domain: general
          metric: gitlab_service_errors:ratio
          severity: s2
          slo_alert: 'yes'
          period: 5m
          bound: upper
          alert_type: symptom
          pager: pagerduty
          deprecated: 'yes'
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has an error-ratio higher than SLOs

            A high proportion of requests to the `{{ $labels.type }}` service are resulting in errors.
            This ratio is higher than the defined SLO for the service.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has an error-ratio exceeding SLO"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "8"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-error-rate.md"
          promql_template_1: 'gitlab_service_errors:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_errors:ratio{environment="$environment", type="$type", stage="$stage"}'

      # Warn: Error ratio SLO not being met, long alert_trigger_duration (15m)
      # Note: we ignore `cny` stage for this alert, since if both cny and main stage are firing, we
      # only care about the main stage alert Alternatively, we have the bad canary alerts
      # specifically for canary stage
      # DEPRECATED in favour of multiwindow, multiburn-rate alerts
      - alert: service_error_ratio_slo_out_of_bounds_upper_15m
        expr: |
          (
            avg(gitlab_service_errors:ratio{stage!="cny", monitor="global"}) by (environment, tier, type, stage)
            > ignoring(environment, stage) group_left
            avg(slo:max:gitlab_service_errors:ratio{alert_trigger_duration="long", monitor="global"}) by (tier, type)
          )
          unless on(tier, type)
          (
            slo:max:events:gitlab_service_errors:ratio{monitor="global"}
          )
        for: 15m
        labels:
          rules_domain: general
          metric: gitlab_service_errors:ratio
          severity: s2
          slo_alert: 'yes'
          period: 15m
          bound: upper
          alert_type: symptom
          pager: pagerduty
          deprecated: 'yes'
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has an error-ratio higher than SLOs

            A high proportion of requests to the `{{ $labels.type }}` service are resulting in errors.
            This ratio is higher than the defined SLO for the service.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) has an error-ratio exceeding SLO"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "8"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-error-rate.md"
          promql_template_1: 'gitlab_service_errors:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_errors:ratio{environment="$environment", type="$type", stage="$stage"}'

      # ------------------------------------------------------------------------------
      # Multiwindow/Multiburn Error Rate Monitoring for COMPONENTS.
      # ------------------------------------------------------------------------------

      # The error rate SLO target for a component is not being met.
      - alert: component_error_ratio_burn_rate_slo_out_of_bounds_upper
        expr: |
          (
            gitlab_component_errors:ratio_1h{monitor="global"} > on(tier, type) group_left() (14.4*(avg(slo:max:events:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)))
          and
            gitlab_component_errors:ratio_5m{monitor="global"} > on(tier, type) group_left()  (14.4*(avg(slo:max:events:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)))
          )
          or
          (
            gitlab_component_errors:ratio_6h{monitor="global"} > on(tier, type) group_left() (6*(avg(slo:max:events:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)))
          and
            gitlab_component_errors:ratio_30m{monitor="global"} > on(tier, type) group_left() (6*(avg(slo:max:events:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)))
          )
        for: 2m
        labels:
          rules_domain: general
          metric: gitlab_component_errors:ratio_1h
          severity: s2
          slo_alert: 'yes'
          period: 2m
          bound: upper
          alert_type: symptom
          pager: pagerduty
        annotations:
          title: "The `{{ $labels.type }}` service, `{{ $labels.component }}` component, `{{ $labels.stage }}` stage, has an error burn-rate exceeding SLO"
          description: >
            The `{{ $labels.type }}` service, `{{ $labels.component }}` component, `{{ $labels.stage }}` stage has an error burn-rate outside of SLO

            The error-burn rate for this service is outside of SLO over multiple windows.
            Currently the error-rate is {{ $value | humanizePercentage }}%.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          grafana_dashboard_id: "alerts-component_multiburn_error/alerts-component-multi-window-multi-burn-rate-out-of-slo"
          grafana_panel_id: "4"
          grafana_variables: "environment,type,stage,component"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-error-rate.md"
          promql_template_1: 'gitlab_component_errors:ratio_5m{environment="$environment", type="$type", stage="$stage", component="$component"}'

      ################################################
      # Operation Rate: how many operations is this service handling per second?
      ################################################

      # ------------------------------------
      # Upper bound thresholds exceeded
      # ------------------------------------

      # Warn: Operation rate above 2 sigma
      - alert: service_ops_out_of_bounds_upper_5m
        expr: |
          (
            (
              (gitlab_service_ops:rate{monitor="global"} -  gitlab_service_ops:rate:prediction{monitor="global"}) /
              gitlab_service_ops:rate:stddev_over_time_1w{monitor="global"}
            )
            >
            3
          )
          unless on(tier, type)
          gitlab_service:mapping:disable_ops_rate_prediction
        for: 5m
        labels:
          rules_domain: general
          metric: gitlab_service_ops:rate
          severity: s4
          period: 5m
          bound: upper
          threshold_sigma: "3"
          alert_type: cause
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is receiving more requests than normal.

            This is often caused by user generated traffic, sometimes abuse. It can also be cause by
            application changes that lead to higher operations rates or from retries in the event of
            errors. Check the abuse reporting watches in Elastic, ELK for possible abuse,
            error rates (possibly on upstream services) for root cause.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "Anomaly detection: The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is receiving more requests than normal"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "10"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "12"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-ops-rate.md"
          promql_template_1: 'gitlab_service_ops:rate{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_ops:rate{environment="$environment", type="$type", stage="$stage"}'

      # ------------------------------------
      # Lower bound thresholds exceeded
      # ------------------------------------

      # Warn: Operation rate below 2 sigma
      - alert: service_ops_out_of_bounds_lower_5m
        expr: |
          (
            (
              (gitlab_service_ops:rate{monitor="global"} -  gitlab_service_ops:rate:prediction{monitor="global"}) /
              gitlab_service_ops:rate:stddev_over_time_1w{monitor="global"}
            )
            <
            -3
          )
          unless on(tier, type)
          gitlab_service:mapping:disable_ops_rate_prediction
        for: 5m
        labels:
          rules_domain: general
          metric: gitlab_service_ops:rate
          severity: s4
          period: 5m
          bound: lower
          threshold_sigma: "3"
          alert_type: cause
        annotations:
          description: >
            The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is receiving fewer requests than normal.

            This is often caused by a failure in an upstream service - for example, an upstream load balancer rejected
            all incoming traffic. In many cases, this is as serious or more serious than a traffic spike. Check
            upstream services for errors that may be leading to traffic flow issues in downstream services.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "Anomaly detection: The `{{ $labels.type }}` service (`{{ $labels.stage }}` stage) is receiving fewer requests than normal"
          grafana_dashboard_id: "general-service/service-platform-metrics"
          grafana_panel_id: "10"
          grafana_variables: "environment,type,stage"
          grafana_min_zoom_hours: "12"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-ops-rate.md"
          promql_template_1: 'gitlab_service_ops:rate{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_ops:rate{environment="$environment", type="$type", stage="$stage"}'

      ################################################
      # Bad canary: we are experiencing errors or latency issues in
      # canary, but not in production. This probably indicates that
      # we have a dud canary
      #
      # When traffic volume to the canary is below 1% of the
      # traffic to the main production stage, the bad-canary
      # alerts will not fire. This avoids low-traffic
      # random noise alerts.
      #
      ################################################
      # DEPRECATED in favour of multiwindow, multiburn-rate alerts

      - alert: service_cny_apdex_slo_out_of_bounds_lower_5m
        expr: |
          (
            (
              (
                avg(gitlab_service_apdex:ratio{stage="cny", monitor="global"}) by (environment, tier, type)
                < ignoring(environment, stage) group_left
                  avg(slo:min:gitlab_service_apdex:ratio{monitor="global"}) by (tier, type)
              )
              unless ignoring(stage)
              (
                avg(gitlab_service_apdex:ratio{stage="main", monitor="global"}) by (environment, tier, type)
                < ignoring(environment, stage) group_left
                  avg(slo:min:gitlab_service_apdex:ratio{monitor="global"}) by (tier, type)
              )
            )
            and on(environment, tier, type)
            (
              (
                gitlab_service_ops:rate{stage="cny", monitor="global"}
                / ignoring(stage)
                gitlab_service_ops:rate{stage="main", monitor="global"}
              ) >= 0.01
            )
          )
          unless on(tier, type)
          (
            slo:min:events:gitlab_service_apdex:ratio{monitor="global"}
          )
        for: 5m
        labels:
          rules_domain: general
          canary_warning: 'yes'
          metric: gitlab_service_apdex:ratio
          severity: s2
          slo_alert: 'yes'
          period: 5m
          bound: lower
          pager: pagerduty
          alert_type: symptom
          deprecated: 'yes'
        annotations:
          description: >
            The `cny` stage of  the `{{ $labels.type }}` service has a apdex score
            (latency) below SLO, but the main stage does not.

            While there are other reasons, such as high traffic to the canary stage, experiencing a
            high error rate in `cny`, without any corresponding errors in `main` stage could indicate
            a malfunctioning canary deploy.

            Consider investigating further. If there is no evidence of another cause, please
            consider stopping the deployment process while the problem is investigated.

            This could indicate that the canary deployment is not functioning correctly. Please consider
            stopping the deployment process while the problem is investigated.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "Bad canary? The `cny` stage of  the `{{ $labels.type }}` service has a apdex score (latency) below SLO, but the main stage does not."
          grafana_dashboard_id: "general-service-stages/general-service-platform-metrics-stages"
          grafana_panel_id: "2"
          grafana_variables: "environment,type"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-apdex.md"
          promql_template_1: 'gitlab_service_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_apdex:ratio{environment="$environment", type="$type", stage="$stage"}'

      - alert: service_cny_error_ratio_slo_out_of_bounds_upper_5m
        expr: |
          (
            (
              (
                avg(gitlab_service_errors:ratio{stage="cny", monitor="global"}) by (environment, tier, type, stage)
                > ignoring(environment, stage) group_left
                avg(slo:max:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)
              )
              unless ignoring(stage)
              (
                avg(gitlab_service_errors:ratio{stage="main", monitor="global"}) by (environment, tier, type, stage)
                > ignoring(environment, stage) group_left
                avg(slo:max:gitlab_service_errors:ratio{monitor="global"}) by (tier, type)
              )
            )
            and on(environment, tier, type)
            (
              (
                gitlab_service_ops:rate{stage="cny", monitor="global"}
                / ignoring(stage)
                gitlab_service_ops:rate{stage="main", monitor="global"}
              ) >= 0.01
            )
          )
          unless on(tier, type)
          (
            slo:max:events:gitlab_service_errors:ratio{monitor="global"}
          )
        for: 5m
        labels:
          rules_domain: general
          canary_warning: 'yes'
          metric: gitlab_service_errors:ratio
          severity: s2
          slo_alert: 'yes'
          period: 5m
          bound: upper
          pager: pagerduty
          alert_type: symptom
          deprecated: 'yes'
        annotations:
          description: >
            The `cny` stage of  the `{{ $labels.type }}` service has an error-ratio
            exceeding SLO, but the main stage does not.

            While there are other reasons, such as high traffic to the canary stage, experiencing a
            high error rate in `cny`, without any corresponding errors in `main` stage could indicate
            a malfunctioning canary deploy.

            Consider investigating further. If there is no evidence of another cause, please
            consider stopping the deployment process while the problem is investigated.
          runbook: "docs/{{ $labels.type }}/service-{{ $labels.type }}.md"
          title: "Bad canary? The `cny` stage of  the `{{ $labels.type }}` service has an error-ratio exceeding SLO, but the main stage does not."
          grafana_dashboard_id: "general-service-stages/general-service-platform-metrics-stages"
          grafana_panel_id: "3"
          grafana_variables: "environment,type"
          grafana_min_zoom_hours: "6"
          link1_title: "Definition"
          link1_url: "https://gitlab.com/gitlab-com/runbooks/blob/master/docs/uncategorized/definition-service-error-rate.md"
          promql_template_1: 'gitlab_service_errors:ratio{environment="$environment", type="$type", stage="$stage"}'
          promql_template_2: 'gitlab_component_errors:ratio{environment="$environment", type="$type", stage="$stage"}'

