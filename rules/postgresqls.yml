groups:
- name: postgresql.rules
  rules:
  - alert: PostgreSQL_WALEReplicationStopped
    expr: rate(pg_xlog_position_bytes{type=~"postgres-(archive|delayed)"}[1m]) == 0
    # Given that long running queries can't temporarily halt replication,
    # we're setting the "for" value to the value of statement_timeout.
    for: 15m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: WAL-E replication seems to be stopped on {{$labels.instance}}.
        Check if PostgreSQL isn't stuck on restoring a WAL segment or if it's capabale
        of downloading WAL segments.
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: WAL-E replication has stopped
  - alert: PostgreSQL_CommitRateTooLow
    expr: rate(pg_stat_database_xact_commit{datname="gitlabhq_production", type =~"postgres|patroni", environment!="gstg"}[1m])
      < 1000
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: Commits/s on {{$labels.instance}} database {{$labels.datname}}
        is {{$value | printf "%.0f" }} which is implausibly low. Perhaps the application
        is unable to connect
      runbook: docs/patroni/postgres.md#availability
      title: Postgres seems to be processing very few transactions
  - alert: PostgreSQL_ConnectionsTooHigh
    expr: sum(pg_stat_activity_count) BY (environment, fqdn) > ON(fqdn) pg_settings_max_connections
      * 0.95
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      runbook: docs/patroni/postgres.md#connections
      title: Postgres has {{$value}} connections on {{$labels.fqdn}} which is close
        to the maximum
  - alert: PostgreSQL_RollbackRateTooHigh
    expr: |
      rate(pg_stat_database_xact_rollback{datname="gitlabhq_production", type="patroni"}[5m])
      / ON(instance, datname) rate(pg_stat_database_xact_commit{datname="gitlabhq_production", type="patroni"}[5m])
      > 0.1
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: Ratio of transactions being aborted compared to committed is {{
        $value | printf "%.2f" }} on {{$labels.instance}}
      runbook: docs/patroni/postgres.md#errors
      title: Postgres transaction rollback rate is high
  - alert: PostgreSQL_StatementTimeout_Errors
    expr: rate(postgresql_errors_total{type="statement_timeout"}[5m]) > 3
    for: 5m
    labels:
      severity: s1
      pager: pagerduty
      alert_type: cause
    annotations:
      descrition: Database {{$labels.fqdn}} has {{ $value | printf "%.1f" }} statement
        timeouts per second
      runbook: docs/patroni/postgres.md#errors
      title: Postgres transactions showing high rate of statement timeouts
  - alert: PostgreSQL_Deadlock_Errors
    expr: rate(postgresql_errors_total{type="deadlock_detected"}[1m]) * 60 > 5
    labels:
      severity: s4
      alert_type: cause
    annotations:
      descrition: Database {{$labels.fqdn}} has {{ $value | printf "%.1f" }} deadlock
        errors per minute
      runbook: docs/patroni/postgres.md#errors
      title: Postgres detected deadlocks
  - alert: PostgreSQL_ReplicationLagTooLarge
    expr: (pg_replication_lag > 180) and ON(instance) (pg_replication_is_replica{type =~"postgres|patroni"} == 1)
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: Replication lag on server {{$labels.instance}} is currently {{
        $value | humanizeDuration }}
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: Postgres Replication lag is over 2 minutes
  - alert: PostgreSQL_ReplicationLagTooLarge_ArchiveReplica
    expr: (pg_replication_lag > 10800) and ON(instance) (pg_replication_is_replica{type = "postgres-archive"} == 1)
    for: 5m
    labels:
      severity: s3
      alert_type: cause
    annotations:
      description: Replication lag on server {{$labels.instance}} is currently {{
        $value | humanizeDuration }}
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: Postgres Replication lag is over 3 hours on archive recovery replica
  - alert: PostgreSQL_ReplicationLagTooLarge_DelayedReplica
    expr: (pg_replication_lag > 32400) and ON(instance) (pg_replication_is_replica{type = "postgres-delayed"} == 1)
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: Replication lag on server {{$labels.instance}} is currently {{
        $value | humanizeDuration }}
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: Postgres Replication lag is over 9 hours on delayed replica (normal is 8 hours)
  - alert: PostgreSQL_ReplicationLagBytesTooLarge
    expr: (pg_xlog_position_bytes and pg_replication_is_replica == 0) - ON(environment)
      GROUP_RIGHT(instance) (pg_xlog_position_bytes and pg_replication_is_replica{type =~"postgres|patroni"}
      == 1) > 1e+09
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: Replication lag on server {{$labels.instance}} is currently {{
        $value | humanize1024}}B
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: Postgres Replication lag (in bytes) is high
  - alert: PostgreSQL_XLOGConsumptionTooHigh
    expr: rate(pg_xlog_position_bytes[2m]) > 36700160 and ON(instance) (pg_replication_is_replica == 0)
    for: 20m
    labels:
      severity: s3
      alert_type: cause
    annotations:
      description: XLOG is being generated at a rate of {{ $value | humanize1024 }}B/s
        on {{$labels.instance}}
      runbook: docs/patroni/postgres.md#replication-is-lagging-or-has-stopped
      title: Postgres is generating XLOG too fast, expect this to cause replication
        lag
  - alert: PostgreSQL_UnusedReplicationSlot
    expr: pg_replication_slots_active == 0
    for: 30m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: Unused {{$labels.slot_type}} slot "{{$labels.slot_name}}" on {{$labels.fqdn}}
      runbook: "docs/patroni/postgres.md#replication_slots"
      title: Unused Replication Slots for {{$labels.fqdn}}
  - alert: PostgreSQL_ReplicaStaleXmin
    expr: pg_replication_slots_xmin_age{slot_name=~"^patroni_[0-9]+.*"} > 20000
    for: 45m
    labels:
      severity: s3
      alert_type: cause
    annotations:
      title: |
        PostgreSQL replication slot {{$labels.slot_name}} on {{$labels.fqdn}} is
        falling behind.
      description: >
        The replication slot {{$labels.slot_name}} on {{$labels.fqdn}} is using
        a minimum transaction ID that is {{$value | humanize}} transactions old.
        This can cause an increase in dead tuples on the primary. This can be
        caused by long-running transactions on the master or any standby, or unused replication.
      runbook: docs/patroni/postgres.md#tables-with-a-large-amount-of-dead-tuples
  - alert: PostgreSQL_LongLivedTransaction
    expr: pg_stat_activity_max_tx_duration > 3600
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: >
        "Postgres server {{$labels.fqdn}} has a transaction in currently in
        state "{{$labels.state}}" that started {{ $value | humanizeDuration }}
        ago"
      runbook: docs/patroni/postgres.md#tables-with-a-large-amount-of-dead-tuples
      title: "There's a long-lived Postgres transaction"
  - alert: PostgreSQL_DBHeavyLoad
    expr: node_load1{type=~"postgres|patroni"} > 200
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: >
        Really high load in the database for the last minute, there are
          {{ query "sum(pg_slow_queries_total)" }} slow queries,
          {{ query "sum(pg_blocked_queries_total)" }} and
          {{ query "sum(pg_locks_count{datname=\"gitlabhq_production\"})" }} locks.
          Check http://dashboards.gitlab.net/dashboard/db/postgres-stats and
          http://dashboards.gitlab.net/dashboard/db/postgres-queries to get
          more data.
      runbook: "docs/patroni/postgres.md#load"
      title: 'High load in database {{ $labels.fqdn }}: {{$value}}'
  - alert: PostgreSQL_TooManyDeadTuples
    expr: >
      (
        (pg_stat_user_tables_n_dead_tup > 1000000)
        /
        (pg_stat_user_tables_n_live_tup + pg_stat_user_tables_n_dead_tup)
      ) >= 0.05
      unless ON(instance) (pg_replication_is_replica == 1)
    for: 30m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: "The dead tuple ratio of {{$labels.relname}} is greater than 5%"
      table: "{{$labels.relname}}"
      dead_tuples: "{{$value}}"
      runbook: "docs/patroni/postgres.md#tables-with-a-large-amount-of-dead-tuples"
      title: PostgreSQL dead tuples is too large
  - alert: PostgreSQL_SplitBrain
    expr: count(pg_replication_is_replica == 0) BY (environment) != 1
    for: 1m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: 'The following {{$value}} databases are in read-write
      (primary) mode: {{range query
        "pg_replication_is_replica{environment=''$labels.environment''} ==
        0.0"}} {{.labels.fqdn}} {{end}}'
      runbook: "docs/patroni/patroni-management.md"
      title: "Split Brain: too many postgres databases in environment {{$labels.environment}}
        in read-write (primary) mode"
  - alert: PostgreSQL_SplitBrain_Replicas
    expr: count(count(pg_stat_wal_receiver_status{type=~"postgres|patroni"} >= 0) BY (environment, upstream_host))
      BY (environment) > 1
    for: 1m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: 'In environment {{$labels.environment}} there are {{$value}} different
        upstream primaries: {{range query "pg_stat_wal_receiver_status{environment=''$labels.environment''}
        >= 0"}}"{{.labels.fqdn}}" thinks "{{.labels.upstream_host}}" is the primary;
        {{end}} '
      runbook: docs/patroni/patroni-management.md
      title: Replicas have different upstream primary databases
  - alert: PostgreSQL_ServiceDown
    expr: pg_up == 0
    for: 1m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: >
        {{$labels.fqdn}} postgres service appears down
      runbook: "docs/patroni/postgres.md"
      title: |
        {{$labels.fqdn}} postgres service appears down
  - alert: PostgreSQL_ServiceRestarted
    expr: time() - pg_postmaster_start_time_seconds < 90
    for: 1m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: >
        {{$labels.fqdn}} postgres service appears to be restarted
      runbook: "docs/patroni/postgres.md"
      title: |
        {{$labels.fqdn}} postgres service appears to be restarted
  - alert: PostgreSQL_RoleChange
    expr: pg_replication_is_replica and changes(pg_replication_is_replica[5m]) > 0
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: Database on {{$labels.fqdn}} changed role to {{if eq $value 1.0}}
        *replica* {{else}} *primary* {{end}}
      title: Postgres Database replica promotion occurred in environment "{{$labels.environment}}"
  - alert: PostgreSQL_ConfigurationChange
    expr: '{__name__=~"pg_settings_.*"} != ON(__name__, fqdn) {__name__=~"pg_settings_([^t]|t[^r]|tr[^a]|tra[^n]|tran[^s]|trans[^a]|transa[^c]|transac[^t]|transact[^i]|transacti[^o]|transactio[^n]|transaction[^_]|transaction_[^r]|transaction_r[^e]|transaction_re[^a]|transaction_rea[^d]|transaction_read[^_]|transaction_read_[^o]|transaction_read_o[^n]|transaction_read_on[^l]|transaction_read_onl[^y]).*"}
      OFFSET 10m'
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: Database on {{$labels.fqdn}} setting now {{$labels.__name__}}={{$value}}
      title: Postgres Database configuration change has occurred on "{{$labels.fqdn}}"
  - alert: PostgreSQL_TooFewPrometheusScrapes
    expr: rate(pg_exporter_scrapes_total[1m]) < 1 / 60
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: '{{$labels.fqdn}} is showing only {{$value}} scrapes per second
        which should be > 0.2'
      title: PostgreSQL Exporter not being scraped
  - alert: PostgreSQL_ExporterErrors
    expr: pg_exporter_last_scrape_error{environment=~"gprd|gstg"} == 1
    for: 1h
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: This may indicate postgres_exporter is not running or a buggy query
        in query.yaml on {{$labels.fqdn}}
      title: Postgres exporter is showing errors for the last hour
  - alert: PostgreSQL_PGBouncer_Errors
    expr: rate(pgbouncer_errors_count{environment=~"gprd|gstg", errmsg!="server conn crashed?"}[10m]) > 5
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      message: "{{$labels.errmsg}}"
      description: PGBouncer is logging errors. This may be due to a a server restart or an admin typing commands at the pgbouncer console
      runbook: docs/patroni/postgres.md#pgbouncer-errors
      title: PGBouncer is logging errors
  - alert: PostgreSQL_PGBouncer_maxclient_conn
    expr: rate(pgbouncer_errors_count{errmsg="no more connections allowed (max_client_conn)"}[1m]) > 0
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: The number of client connections has reached max_client_conn.
        Consider raising this parameter or investigating the source of the unusually large number of connections
      runbook: docs/patroni/postgres.md#pgbouncer-errors
      title: PGBouncer is logging "no more connections allowed" errors
  - alert: PostgreSQL_too_many_slow_queries
    expr: rate(pg_slow_queries{environment=~"gprd"}[1m]) * 60 > 300
    for: 5m
    labels:
      pager: pagerduty
      severity: s1
      alert_type: cause
    annotations:
      description: >
        Database {{ $labels.fqdn }} has {{ $value | printf "%.1f" }} slow
        queries. This may be the result of badly written SQL queries, an
        overloaded database, or a performance regression introduced in a new
        release.
      title: There are too many slow SQL queries
  - alert: PostgreSQL_SSLCompressionActive
    expr: sum(pg_stat_ssl_compression{environment=~".*prd"}) BY (fqdn) > 0
    for: 10m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: >
        Database {{ $labels.fqdn }} has {{ $value | printf "%.1f" }}
        connections with SSL compression enabled. This may add significant
        jitter in replication delay. Replicas should turn off SSL compression
        via `sslcompression=0` in `recovery.conf`.
      title: Clients are using SSL compression
  - alert: PostgreSQL_HighRateOfTempFilesCreation
    expr: rate(postgresql_temp_files_total[1m]) > 1
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: >
        Database {{ $labels.fqdn }} has a high rate of temporary files creation,
        which can be a result of an abuse or poorly-written query.
      title: High rate of temporary files creation
  - alert: PostgreSQL_TooManyLocksAcquired
    expr: ((sum by(fqdn) (pg_locks_count)) / on (fqdn) ( pg_settings_max_locks_per_transaction * pg_settings_max_connections )) > 0.20
    for: 5m
    labels:
      severity: s4
      alert_type: cause
    annotations:
      description: >
        Too many locks acquired on the database instance {{ $labels.fqdn }}.
        If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction.
      title: Too many locks acquired
